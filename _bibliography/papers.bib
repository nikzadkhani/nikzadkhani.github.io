---
---
@inproceedings{khani-etal-2021-cultural,
    bibtex_show = {true},
    abbr = {NAACL},
    code = {https://github.com/nikzadkhani/MMID-CNN-Analysis},
    pdf = {https://aclanthology.org/2021.naacl-main.19.pdf},
    title = "Cultural and Geographical Influences on Image Translatability of Words across Languages",
    author = "Khani, Nikzad  and
      Tourni, Isidora  and
      Rasooli, Mohammad Sadegh  and
      Callison-Burch, Chris  and
      Wijaya, Derry Tanti",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.19",
    doi = "10.18653/v1/2021.naacl-main.19",
    pages = "198--209",
    selected={true},
    abstract = "Neural Machine Translation (NMT) models have been observed to produce poor translations when there are few/no parallel sentences to train the models. In the absence of parallel data, several approaches have turned to the use of images to learn translations. Since images of words, e.g., horse may be unchanged across languages, translations can be identified via images associated with words in different languages that have a high degree of visual similarity. However, translating via images has been shown to improve upon text-only models only marginally. To better understand when images are useful for translation, we study image translatability of words, which we define as the translatability of words via images, by measuring intra- and inter-cluster similarities of image representations of words that are translations of each other. We find that images of words are not always invariant across languages, and that language pairs with shared culture, meaning having either a common language family, ethnicity or religion, have improved image translatability (i.e., have more similar images for similar words) compared to its converse, regardless of their geographic proximity. In addition, in line with previous works that show images help more in translating concrete words, we found that concrete words have improved image translatability compared to abstract ones.",
}

@misc{khani2021exploration,
    title={An Exploration of Deep Learning Methods in Hungry Geese},
    author={Nikzad Khani and Matthew Kluska},
    year={2021},
    eprint={2109.01954},
    archivePrefix={arXiv},
    primaryClass={cs.AI},
    bibtex_show={true},
    abbr={arXiv},
    pdf={http://export.arxiv.org/pdf/2109.01954},
    code={https://github.com/nikzadkhani/Famished-Geese},
    abstract={Hungry Geese is a n-player variation of the popular game snake.   This paperlooks at state of the art Deep Reinforcement Learning Value Methods. The goalof the paper is to aggregate research of value based methods and apply it as anexercise to other environments. A vanilla Deep Q Network, a Double Q-networkand a Dueling Q-Network were all examined and tested with the Hungry Geeseenvironment. The best performing model was the vanilla Deep Q Network due to itssimple state representation and smaller network structure. Converging towards anoptimal policy was found to be difficult due to random geese initialization and foodgeneration. Therefore we show that Deep Q Networks may not be the appropriatemodel for such a stochastic environment and lastly we present improvements thatcan be made along with more suitable models for the environment.}
}
