I"<h2 id="intuition">Intuition</h2>
<p>The main goal of q-learning and really a lot of other reinforcement learning algorithms is to learn an optimal policy which is
usually symbolized as $\pi$. This boils down to finding the action in every scenario that will give us the best outcome. The
policy that does this the best is called the optimal policy which is usually symbolized as $\pi^*$. Q-learning is in algorithm
to find this policy using what is called a $Q$ function.</p>

<h2 id="q-function">$Q$ Function</h2>
<p>The $Q$ function is some function that we need to learn throught training that takes in a <em>state</em>, $s$, and an <em>action</em>, $a$, and will return the expected value of the reward. Note that the expected value will take into account the rewards of all future state-action pairs that branch after the given action.</p>

<p>There are a lot of different ways to learn and capture the $Q$-function. One of the most basic ways is to use a table, where
the row numbers represent different states in our environment and the columns numbers represent the actions we can take, then the
entry of the table will be the expected reward. If our Q-table perfectly captured the expected rewards for the environment, then
the optimal policy would simply entail querying the row number for the state and choosing the action whose column has the
greatest expected reward. In reality, more often than not, our expected rewards will have some error attributed to them, so our
performance will not be perfect.</p>

<h2 id="updating-the-q-function">Updating the $Q$ Function</h2>
<p>For every step in our episode we can update our $Q$-function according to the following formula</p>

<p>\begin{equation}
Q(s,a) = (1 - \alpha) * Q(s,a) + \alpha * (r + \gamma * Q(s’, a’))
\end{equation}</p>
:ET